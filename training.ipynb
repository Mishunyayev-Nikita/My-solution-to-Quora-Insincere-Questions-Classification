{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "from utils import Attention, CyclicLR\n",
    "from rnn import NeuralNet_2\n",
    "from rcnn import NeuralNet_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "85ee6b26944da2cd972c5038fa63c78b763184d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 55 # max number of words in a question to use\n",
    "\n",
    "batch_size = 512\n",
    "train_epochs = 3\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "5013e3945f2454854337ff3ee7eee98efa79fd25",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        # allows python to do it faster\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2fa12bd9b5e0d322cd5b3cc3e3f7a4c4ab5fe496",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_meta_features(df, text_column):    \n",
    "    df['length'] = df[text_column].apply(lambda x: len(x))\n",
    "    df['num_words'] = df[text_column].apply(lambda x: len(x.split()))\n",
    "    df['num_unique_words'] = df[text_column].apply(lambda x: len(set(w for w in x.split())))\n",
    "    df['capitals'] = df[text_column].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    #df['num_puncts'] = df[text_column].apply(lambda x: sum(x.count(p) for p in puncts if p in x))\n",
    "    \n",
    "    #df['num_smilies'] = df[text_column].apply(lambda x: sum(x.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    #df['num_sad'] = df[text_column].apply(lambda x: sum(x.count(w) for w in (':-<', ':()', ';-()', ';(')))\n",
    "    \n",
    "    df['caps_vs_length'] = df['capitals'] / df['length']\n",
    "    df['words_vs_unique_words'] = df['num_words'] / df['num_unique_words']\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df[['caps_vs_length', 'words_vs_unique_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "959cc1e87100b24795b8a8b87901b5e9e5d12ec7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "45065a2ed168d10e6dc07fdc1e9bc1f826b34c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 15.297364711761475\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Get meta features\n",
    "train_features = get_meta_features(train_df, 'question_text')\n",
    "test_features = get_meta_features(test_df, 'question_text')\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_features.values)\n",
    "train_features = ss.transform(train_features)\n",
    "test_features = ss.transform(test_features)\n",
    "\n",
    "finish = time.time()\n",
    "print('Elapsed time:', finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "944f886420a9fdb2aba8cea8c367a1fb946dc57f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "0929f25eda4df617575e8faa515598a169b26f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 50.44481563568115\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "## Tokenize the sentences\n",
    "train_X = train_df[\"question_text\"].values\n",
    "test_X = test_df[\"question_text\"].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, filters=[])\n",
    "tokenizer.fit_on_texts(list(train_X) + list(test_X))\n",
    "word_index = tokenizer.word_index\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "\n",
    "#shuffling the data\n",
    "np.random.seed(SEED)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "train_X = train_X[trn_idx]\n",
    "train_y = train_y[trn_idx]\n",
    "\n",
    "finish = time.time()\n",
    "print('Elapsed time:', finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4a0c118fe9222918309254b3758965e9f6b54c94",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "84a3c0758a21d8cedb19090cc15041cbb2abec95",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f19a7f89db3f046ef3efa2ec23d1e914bd80b737"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95000, 300)\n",
      "Elapsed time: 351.6205518245697\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix_2 = load_para(word_index)\n",
    "\n",
    "embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2], axis=0)\n",
    "print(np.shape(embedding_matrix))\n",
    "\n",
    "del embedding_matrix_1, embedding_matrix_2\n",
    "gc.collect()\n",
    "\n",
    "finish = time.time()\n",
    "print('Elapsed time:', finish - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d96b559a466a804d7a7bc7489b2f5e03e4e2114"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "4bc17062b48b74557b5e4d7b94014dd3a5f87e48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "1cf4c6c6ae2afe7b3f7dbf6688db984a771be9a8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scoring(y_true, y_proba, verbose=True):\n",
    "    from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "    def threshold_search(y_true, y_proba):\n",
    "        precision , recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        thresholds = np.append(thresholds, 1.001)\n",
    "        with np.errstate(divide='ignore'):\n",
    "            F = 2 / (1/precision + 1/recall)\n",
    "        best_score = np.max(F)\n",
    "        best_th = thresholds[np.argmax(F)]\n",
    "        return best_th \n",
    "\n",
    "\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "    scores = []\n",
    "    ths = []\n",
    "    for train_index, test_index in rkf.split(y_true, y_true):\n",
    "        y_prob_train, y_prob_test = y_proba[train_index], y_proba[test_index]\n",
    "        y_true_train, y_true_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "        # determine best threshold on 'train' part \n",
    "        best_threshold = threshold_search(y_true_train, y_prob_train)\n",
    "\n",
    "        # use this threshold on 'test' part for score \n",
    "        sc = f1_score(y_true_test, (y_prob_test >= best_threshold).astype(int))\n",
    "        scores.append(sc)\n",
    "        ths.append(best_threshold)\n",
    "\n",
    "    best_th = np.mean(ths)\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    if verbose: print(f'Best threshold: {np.round(best_th, 4)}, Score: {np.round(score,5)}')\n",
    "\n",
    "    return best_th, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "b6c179e5478f1ac92b69020fb875bf3d8154bff3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "60baed27ed807cd5b3dc3bcfc7bad9767b63a67a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f50bdf1812901f932f3d4361ab2f794675cdc2f"
   },
   "source": [
    "## Training 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "1f80aa8c099cb76eede9c0c5db859a835fc9d1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/3 \t loss=0.1425 \t val_loss=0.1042 \t val_f1=0.6626 val_th=0.42 \t time=144.59s\n",
      "Epoch 2/3 \t loss=0.1130 \t val_loss=0.0978 \t val_f1=0.6856 val_th=0.32 \t time=144.72s\n",
      "Epoch 3/3 \t loss=0.1064 \t val_loss=0.0965 \t val_f1=0.6920 val_th=0.29 \t time=144.47s\n",
      "Fold 2\n",
      "Epoch 1/3 \t loss=0.1358 \t val_loss=0.1036 \t val_f1=0.6690 val_th=0.24 \t time=144.34s\n",
      "Epoch 2/3 \t loss=0.1123 \t val_loss=0.0977 \t val_f1=0.6803 val_th=0.31 \t time=144.61s\n",
      "Epoch 3/3 \t loss=0.1062 \t val_loss=0.0973 \t val_f1=0.6876 val_th=0.29 \t time=144.56s\n",
      "Fold 3\n",
      "Epoch 1/3 \t loss=0.1390 \t val_loss=0.1054 \t val_f1=0.6627 val_th=0.30 \t time=144.61s\n",
      "Epoch 2/3 \t loss=0.1131 \t val_loss=0.1018 \t val_f1=0.6746 val_th=0.29 \t time=144.73s\n",
      "Epoch 3/3 \t loss=0.1064 \t val_loss=0.0998 \t val_f1=0.6829 val_th=0.29 \t time=144.64s\n",
      "Fold 4\n",
      "Epoch 1/3 \t loss=0.1329 \t val_loss=0.1027 \t val_f1=0.6691 val_th=0.29 \t time=144.55s\n",
      "Epoch 2/3 \t loss=0.1123 \t val_loss=0.0991 \t val_f1=0.6823 val_th=0.27 \t time=144.95s\n",
      "Epoch 3/3 \t loss=0.1061 \t val_loss=0.0964 \t val_f1=0.6916 val_th=0.31 \t time=145.12s\n",
      "Fold 5\n",
      "Epoch 1/3 \t loss=0.1365 \t val_loss=0.1032 \t val_f1=0.6647 val_th=0.35 \t time=144.08s\n",
      "Epoch 2/3 \t loss=0.1124 \t val_loss=0.0985 \t val_f1=0.6788 val_th=0.38 \t time=144.25s\n",
      "Epoch 3/3 \t loss=0.1062 \t val_loss=0.0976 \t val_f1=0.6834 val_th=0.35 \t time=144.16s\n"
     ]
    }
   ],
   "source": [
    "train_preds_2 = np.zeros((len(train_X)))\n",
    "test_preds_2 = np.zeros((len(test_X)))\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_torch(SEED)\n",
    "\n",
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    # split data in train / validation according to the KFold indeces\n",
    "    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n",
    "    features_train_fold = train_features[train_idx]\n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    features_val_fold = train_features[valid_idx]\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    model = NeuralNet_2()\n",
    "    model.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"mean\").cuda()\n",
    "    \n",
    "    step_size = 300\n",
    "    base_lr, max_lr = 0.0005, 0.003\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "    \n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "                         step_size=step_size, mode='exp_range')\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):            \n",
    "            # Forward pass: compute predicted y by passing x to the model\n",
    "            f = features_train_fold[index]\n",
    "            y_pred = model([x_batch, f])\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            \n",
    "            # Compute and print loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights of the model)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test_X))\n",
    "        avg_val_loss = 0.\n",
    "        \n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            f = features_val_fold[index]\n",
    "            y_pred = model([x_batch, f]).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        val_th, val_f1 = scoring(y_val_fold.cpu().numpy(), valid_preds_fold, verbose=False)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} val_th={:.2f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, train_epochs, avg_loss, avg_val_loss, val_f1, val_th, elapsed_time))\n",
    "        \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * batch_size:(i+1) * batch_size]\n",
    "        y_pred = model([x_batch, f]).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds_2[valid_idx] = valid_preds_fold\n",
    "    test_preds_2 += test_preds_fold / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "84e2b12d1a7331764afc2bcc6d27366bd01733b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.3135, Score: 0.68677\n",
      "CPU times: user 18.2 s, sys: 4 ms, total: 18.2 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "th_result, sc = scoring(train_y, train_preds_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50162cf042f2d3f119d994e3220b67025f0b218d"
   },
   "source": [
    "## Training 3rd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "79f7a7a8e471809e17cef0ac6826aa6f3d91b348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/3 \t loss=0.1277 \t val_loss=0.1043 \t val_f1=0.6646 val_th=0.31 \t time=180.83s\n",
      "Epoch 2/3 \t loss=0.1133 \t val_loss=0.0991 \t val_f1=0.6826 val_th=0.39 \t time=181.43s\n",
      "Epoch 3/3 \t loss=0.1070 \t val_loss=0.0973 \t val_f1=0.6899 val_th=0.29 \t time=181.43s\n",
      "Fold 2\n",
      "Epoch 1/3 \t loss=0.1284 \t val_loss=0.1058 \t val_f1=0.6600 val_th=0.33 \t time=181.05s\n",
      "Epoch 2/3 \t loss=0.1130 \t val_loss=0.0995 \t val_f1=0.6774 val_th=0.28 \t time=181.39s\n",
      "Epoch 3/3 \t loss=0.1065 \t val_loss=0.0975 \t val_f1=0.6836 val_th=0.34 \t time=181.36s\n",
      "Fold 3\n",
      "Epoch 1/3 \t loss=0.1283 \t val_loss=0.1092 \t val_f1=0.6576 val_th=0.24 \t time=181.34s\n",
      "Epoch 2/3 \t loss=0.1128 \t val_loss=0.1009 \t val_f1=0.6744 val_th=0.34 \t time=181.44s\n",
      "Epoch 3/3 \t loss=0.1065 \t val_loss=0.0988 \t val_f1=0.6827 val_th=0.42 \t time=181.45s\n",
      "Fold 4\n",
      "Epoch 1/3 \t loss=0.1282 \t val_loss=0.1046 \t val_f1=0.6649 val_th=0.28 \t time=181.28s\n",
      "Epoch 2/3 \t loss=0.1132 \t val_loss=0.0992 \t val_f1=0.6784 val_th=0.32 \t time=181.79s\n",
      "Epoch 3/3 \t loss=0.1068 \t val_loss=0.0976 \t val_f1=0.6854 val_th=0.39 \t time=181.80s\n",
      "Fold 5\n",
      "Epoch 1/3 \t loss=0.1281 \t val_loss=0.1040 \t val_f1=0.6573 val_th=0.33 \t time=181.40s\n",
      "Epoch 2/3 \t loss=0.1128 \t val_loss=0.1018 \t val_f1=0.6754 val_th=0.25 \t time=181.60s\n",
      "Epoch 3/3 \t loss=0.1061 \t val_loss=0.0981 \t val_f1=0.6839 val_th=0.28 \t time=181.68s\n"
     ]
    }
   ],
   "source": [
    "train_preds_3 = np.zeros((len(train_X)))\n",
    "test_preds_3 = np.zeros((len(test_X)))\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_torch(SEED)\n",
    "\n",
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    # split data in train / validation according to the KFold indeces\n",
    "    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n",
    "    #features_train_fold = train_features[train_idx]\n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    #features_val_fold = train_features[valid_idx]\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    model = NeuralNet_3()\n",
    "    model.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"mean\").cuda()\n",
    "    \n",
    "    step_size = 300\n",
    "    base_lr, max_lr = 0.0005, 0.003\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "    \n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "                         step_size=step_size, mode='exp_range')\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):            \n",
    "            # Forward pass: compute predicted y by passing x to the model\n",
    "            #f = features_train_fold[index]\n",
    "            #y_pred = model([x_batch, f])\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            \n",
    "            # Compute and print loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights of the model)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test_X))\n",
    "        avg_val_loss = 0.\n",
    "        \n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            #f = features_val_fold[index]\n",
    "            #y_pred = model([x_batch, f]).detach()\n",
    "            y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        val_th, val_f1 = scoring(y_val_fold.cpu().numpy(), valid_preds_fold, verbose=False)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} val_th={:.2f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, train_epochs, avg_loss, avg_val_loss, val_f1, val_th, elapsed_time))\n",
    "        \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        #f = test_features[i * batch_size:(i+1) * batch_size]\n",
    "        #y_pred = model([x_batch, f]).detach()\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds_3[valid_idx] = valid_preds_fold\n",
    "    test_preds_3 += test_preds_fold / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "84e2b12d1a7331764afc2bcc6d27366bd01733b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.3345, Score: 0.68259\n",
      "CPU times: user 18.5 s, sys: 12 ms, total: 18.5 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "th_result, sc = scoring(train_y, train_preds_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be86c37517979a2eac5412e41326a44a1c13e938"
   },
   "source": [
    "## simple blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "bbdaf77a1650da427d8055efbc165601407542b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.3322, Score: 0.69315\n",
      "CPU times: user 18.6 s, sys: 4 ms, total: 18.6 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "th_result, sc = scoring(train_y, train_preds_3 * 0.5 + train_preds_2 * 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb57a297fe2743defc8187f07ecb9f32dec45aac"
   },
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "fb62a327d4582fa7db2b254b26e7859181cd876b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = test_preds_3 * 0.5 + test_preds_2 * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "c3581f74ae694eb07182e2a23c9db8f01a78f1ba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub.prediction = test_preds > th_result\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
